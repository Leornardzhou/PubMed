---
title: "Analysis of Contributions to PubMed Commons"
author: "Neil Saunders"
date: "28/11/2016"
output: 
  html_document: 
    fig_caption: yes
    highlight: pygments
    theme: flatly
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, tidy = TRUE, warning = FALSE, message = FALSE, fig.path = "../../output/")
library(knitr)
library(readr)
library(lubridate)
library(stringr)
library(dplyr)
library(tidyr)
library(ggplot2)
```

## Introduction
This report analyses data from [PubMed Commons](https://www.ncbi.nlm.nih.gov/pubmedcommons/), a user forum for commenting on scientific articles in the [PubMed database](https://www.ncbi.nlm.nih.gov/pubmed).

## Getting the data
Currently, Commons data is not available via [EUtils](https://www.ncbi.nlm.nih.gov/books/NBK25497/) (the NCBI Entrez API), other than as a PubMed search filter to return only articles that have comments:

```
has_user_comments[Filter]
```

However, a web search for [pubmed common api](https://www.google.com.au/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#newwindow=1&q=pubmed+commons+api) returns [this useful Gist](https://gist.github.com/hubgit/ed04da6ea8a2fca07583). It reveals a URL which returns Commons data for a given PMID in JSON format, for example:

```
https://www.ncbi.nlm.nih.gov/myncbi/comments/?p$rq=CommL.CommServer:com&cmd=get&recid=27424783
```

The value of the _comments_ key is the HTML-formatted list of comments found at the web page for that PMID. It's an unordered list with _id_ and _class_ of _comment\_list_.

The Ruby code at the end of this document was used to generate a list of PMIDs for all articles with Commons comments, retrieve the comment data, parse it and save a summary in CSV format.

## Formatting & cleaning the data
Now we can read the CSV file, parse and format the _date_ column as DateTime and count the up/down votes (if any) for each comment.

The next issue is that not every list item of class _comm\_item_ (see the Ruby code) is a comment on the article. Some of the comments are notes, automatically-generated when the article URL is used in a comment elsewhere, or to indicate activity by a particular user. Others are moderated comments, where the comment text was deleted by the author or a moderator. So we'll label comments as "real" if they contain both an author name and comment text.

```{r get-data}
commons <- read_csv("~/Dropbox/projects/github_projects/PubMed/commons/data/commons.csv", col_names = FALSE)
colnames(commons) <- c("pmid", "pubdate", "cmid", "date", "author.name", "author.href", "votes", "permalink", "text", "moderated")

# extract and format dates
commons$datetime <- ymd_hm(str_match(commons$date, "^(.*?)\\.")[,2])
commons$pubyear <- as.numeric(substr(commons$pubdate, 1, 4))

# up/down votes
commons$votes.total <- as.numeric(str_match(commons$votes, "^(\\d+) of (\\d+) ")[,3])
commons$votes.up <- as.numeric(str_match(commons$votes, "^(\\d+) of (\\d+) ")[,2])
commons$votes.down <- commons$votes.total - commons$votes.up

# mark as 'real comment'
commons$real <- ifelse(!is.na(commons$author.name) & !is.na(commons$text), 1, 0)
commons.real <- subset(commons, real == 1)
```

## Analysis
### Totals

The first chart looks at various "totals". It's not a great chart since it combines unrelated metrics (comments, authors and articles), but it is useful as a summary.

Definitions:

* total.records - total rows in the CSV file of comments
* final.dataset - total "real" comments; those with author name and comment text (_i.e._ not notes, not moderated)
* unique.articles - total unique articles (PMIDs) with at least one comment
* unique.authors - total unique authors who wrote at least one comment
* moderated - total comments deleted by author or a moderator

```{r count-comments}
# all comments
commons.sum <- data.frame(`total records` = nrow(commons), `final dataset` = nrow(commons.real), moderated = nrow(commons[!is.na(commons$moderated), ]), `unique articles` = length(unique(commons.real$pmid)), `unique authors` = length(unique(commons.real$author.name)), stringsAsFactors = FALSE)
commons.sum.tidy <- gather(commons.sum, key, value)
ggplot(commons.sum.tidy, aes(reorder(key, value, max), value)) + geom_bar(stat = "identity", fill = "#336699", width = 0.4) + theme_bw() + geom_text(aes(label = value), vjust = -0.2) + scale_y_continuous(breaks = seq(0, 7000, 1000), limits = c(0, 7000)) + labs(y = "total", x = "category", title = "Total PubMed Commons Comments By Sub-categories")
```

### Comments by date
First, let's look at comments by year. The low number for 2013 reflects the launch dates: the service began as a closed pilot about halfway through the year, opening more widely in October. 2015 saw a decline in comments relative to the previous year, but comments are on the rise again in 2016.

```{r comments-year}
commons.y <- commons.real %>% mutate(year = year(datetime)) %>% group_by(year) %>% summarise(total = n())
ggplot(commons.y, aes(year, total)) + geom_bar(stat = "identity", fill = "#336699", width = 0.4) + theme_bw() + labs(y = "comments", title = "Total PubMed Commons Comments By Year") + geom_text(aes(label = total), vjust = -0.2)
```

Comments by month gives us more detail. The October 2013 service opening date is apparent. Following initial enthusiasm there was a marked decline in comments until the end of 2014, after which monthly totals became more consistent. There are also peaks worthy of further investigation in late 2014 and in August 2016.

```{r comments-month-year}
commons.my <- commons.real %>% mutate(month = month(datetime), year = year(datetime)) %>% group_by(month, year) %>% summarise(total = n())
ggplot(commons.my, aes(month, total)) + geom_bar(stat = "identity", fill = "#336699") + facet_grid(year ~ .) + theme_bw() + scale_x_continuous(breaks = seq(1, 12, 1)) + labs(y = "comments", title = "Total PubMed Commons Comments By Year & Month")
```

Comments by day provides perhaps too much information to be resolved at the scale of this small static chart. However, the events in late 2014 and August 2016 are again apparent.

```{r comments-day-year}
ggplot(commons.real, aes(yday(datetime))) + geom_bar(fill = "#336699") + theme_bw() + facet_grid(year(datetime) ~ .) + labs(x = "day", y = "comments", title = "PubMed Commons Comments By Day & Year")
```

Here are the top 5 days for comments posted.

```{r comments-days-top5}
commons.days <- as.data.frame(table(as.Date(commons.real$datetime)), stringsAsFactors = FALSE)
commons.days <- commons.days[order(commons.days$Freq, decreasing = TRUE), ]
colnames(commons.days) <- c("date", "comments")
kable(head(commons.days, 5), row.names = FALSE)
```

### Comments by year of article publication
There's a clear trend for commenting on more recent articles.

```{r comments-article-date}
low  <- min(commons.real$pubyear, na.rm = TRUE)
high <- max(commons.real$pubyear, na.rm = TRUE)

# earliest PMID with a comment
pmid.first <- commons.real[which(commons.real$pubyear == min(commons.real$pubyear, na.rm = TRUE)), "pmid"]

ggplot(commons.real, aes(pubyear)) + geom_bar(fill = "#336699") + theme_bw() + scale_x_continuous(breaks = seq(low, high, 10)) + labs(x = "year of publication", y = "comments", title = "Total PubMed Commons Comments By Year Of Article Publication")
```

This is even clearer when we plot the distribution of article publication year against year when comments were authored.

Currently, the earliest PubMed article with a comment is [this one](`r paste("https://www.ncbi.nlm.nih.gov/pubmed/", pmid.first, sep = "")`).

```{r comments-article-date_dist}
ggplot(commons.real, aes(factor(year(datetime)), pubyear)) + geom_jitter(color = "#336699", alpha = 0.5) + theme_bw() + labs(x = "year comment posted", y = "year article published", title = "Distribution Of Publication Year For PubMed Articles With Comments") + geom_violin(alpha = 0) + scale_y_continuous(breaks = seq(low, high, 10))
```

### Comments per article
```{r comments-article}
commons.pmid <- commons.real %>% group_by(pmid) %>% summarise(count = n())

# PMID with most comments
pmid.max <- commons.pmid[commons.pmid$count == max(commons.pmid$count), "pmid"]
```

Unsurprisingly, by far the commonest number of comments per article is one. This is followed by a "long-tail" out to a maximum of `r max(commons.pmid$count)`.

Currently, the PubMed article with the most comments is [this one](`r paste("https://www.ncbi.nlm.nih.gov/pubmed/", pmid.max, sep = "")`).

```{r comments-article-plot}
ggplot(commons.pmid, aes(count)) + geom_density(color = "#336699", fill = "#336699") + theme_bw() + labs(x = "comments", title = "Distribution Of PubMed Commons Comments Per Article")
```

### Comments per author
```{r comments-author}
commons.auth <- commons.real %>% group_by(author.name) %>% summarise(count = n())
```

Once again, the most frequent number of comments per author is one, although a respectable proportion of authors have made more than one comment. Currently, the most comments by one author is `r max(commons.auth$count)`.


```{r comments-author-plot}
ggplot(commons.auth, aes(count)) + geom_density(color = "#336699", fill = "#336699") + theme_bw() + labs(x = "comments", title = "Distribution Of PubMed Commons Comments Per Author")
```

### Up/down votes
PubMed Commons has a form of voting for comments, allowing users to vote "yes" or "no" to the question "Was this helpful?"

One way to visualise the votes is simply to plot down votes versus up votes, adding a heatmap of bin counts to deal with overplotting (multiple points with the same values). The plot shows that where comments have votes (currently `r length(na.omit(commons.real$votes.total))` / `r nrow(commons.real)`), the most frequent case is one upvote ("1 of 1 people found this helpful"), followed by 2 or 3 upvotes, no down votes.

```{r votes-density}
ggplot(commons.real, aes(votes.down, votes.up)) + geom_bin2d(bins = 50) + theme_bw() + scale_fill_gradient(low = "yellow", high = "red")

# PMID with most-liked comment
pmid.maxlike <- commons.real[which(commons.real$votes.up == max(commons.real$votes.up, na.rm = TRUE)), "pmid"]
```

There are some clear outliers on this chart with a high number up votes and no down votes. Currently, the PubMed article with the most up-voted comment is [this one](`r paste("https://www.ncbi.nlm.nih.gov/pubmed/", pmid.maxlike, sep = "")`).

### Moderation
Comments deemed inappropriate are wrapped in a _div_ with class = _not\_appr_. The text content of the _div_ indicates whether the comment was deleted by the author (user) or by a moderator. In our CSV file, the text is stored in the _moderated_ column.

```{r comments-mod}
commons.mod <- as.data.frame(table(commons$moderated), stringsAsFactors = FALSE)
ggplot(commons.mod, aes(Var1, Freq)) + geom_bar(stat = "identity", fill = "#336699", width = 0.4) + theme_bw() + geom_text(aes(label = Freq), vjust = -0.2) + labs(x = "removed by", y = "comments", title = "Moderation Of PubMed Commons Comments")
```

## Not yet analysed
There are likely to be other elements of interest in comment lists that we have not yet analysed. For example, some comments are nested in reply to previous comments but this is not captured by the CSS selectors in the current Ruby code. It would be of interest to see how many users reply to a comment, versus "replying" using a top-level comment.

It might also be interesting to analyse comment text using _e.g._ sentiment analysis.

## Supplementary
The Ruby code used to download PubMed Commons data.

```
#!/usr/bin/env ruby

require 'open-uri'
require 'nokogiri'
require 'json'
require 'bio'
require 'csv'

# esearch for articles with comments
Bio::NCBI.default_email = "me@me.com"
ncbi   = Bio::NCBI::REST.new
max    = ncbi.esearch_count("has_user_comments[Filter]", {"db" => "pubmed"})
search = ncbi.esearch("has_user_comments[Filter]", {"db" => "pubmed", "retmax" => max})

# get JSON from NCBI for a PMID
outdir = File.expand_path("../../../data", __FILE__)
output = []
base   = "https://www.ncbi.nlm.nih.gov/myncbi/comments/?p$rq=CommL.CommServer:com&cmd=get&recid="

#search  = search[1..10] # testing

CSV.open("#{outdir}/commons.csv", "wb") do |csv|
  search.each do |pmid|
    puts pmid
    pdate = Bio::MEDLINE.new(Bio::PubMed.query(pmid)).dp
    j = JSON.parse(open(base + pmid.to_s).read)
    d = Nokogiri::HTML(j['comments'])
    d.xpath('//li[starts-with(@class, "comm_item")]').each do |item|
      cmid = item.has_attribute?('data-cmid') == false ? "" : item.attribute('data-cmid')
      auth = item.css('a.comm_f_name').count == 0 ? "" : item.css('a.comm_f_name').inner_text
      href = item.css('a.comm_f_name').count == 0 ? "" : item.css('a.comm_f_name').attribute('href')
      date = item.css('a.comm_date_d').count == 0 ? "" : item.css('a.comm_date_d').inner_text
      text = item.css('div.comm_content').count == 0 ? "" : item.css('div.comm_content').inner_text.gsub("\n", " ")
      mods = item.css('div.not_appr').count == "" ? 0 : item.css('div.not_appr').inner_text
      vote = item.css('span.comm_votes').count == 0 ? "" : item.css('span.comm_votes').inner_text
      link = item.css('a.comm_permalink').count == 0 ? "" : item.css('a.comm_permalink').attribute('href')
      csv << [pmid, pdate, cmid, date, auth, href, vote, link, text, mods]
    end
  end
end
```